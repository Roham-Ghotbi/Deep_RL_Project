import numpy as np

from cs285.infrastructure.dqn_utils import MemoryOptimizedReplayBuffer, PiecewiseSchedule
from cs285.policies.ActorPolicy import ActorPolicy
from cs285.policies.argmax_policy import ArgMaxPolicy
from cs285.critics.diffq_critic import DiffQCritic
from cs285.critics.diffq_critic_base import DiffQCriticBase
from cs285.exploration.rnd_model import RNDModel

import torch
from cs285.infrastructure import pytorch_util as ptu


class Explore_Exploit_agent(object):
    def __init__(self, env, agent_params):

        self.env = env
        self.agent_params = agent_params
        self.batch_size = agent_params['batch_size']
        self.noise_ratio = self.agent_params['observation_noise_multiple']
        self.last_obs = self.apply_noise(self.env.reset(), self.noise_ratio)

        self.num_actions = agent_params['ac_dim']
        self.learning_starts = agent_params['learning_starts']
        self.learning_freq = agent_params['learning_freq']
        self.target_update_freq = agent_params['target_update_freq']
        self.discrete = agent_params['discrete']

        self.replay_buffer_idx = None


        self.critic = DiffQCritic(agent_params) if not agent_params['diff_training_disabled'] else DiffQCriticBase(agent_params)

        ## Exploration
        self.exploration = agent_params['exploration_schedule']
        self.num_exploration_steps = agent_params['num_exploration_steps']
        self.exploration_critic = DiffQCriticBase(agent_params)
        self.exploration_model = RNDModel(agent_params)
        self.explore_weight_schedule = agent_params['exploration_schedule']
        self.expl_actor = ArgMaxPolicy(self.exploration_critic) if self.discrete else ActorPolicy(agent_params['ac_dim'],
                                                                                                  agent_params['ob_dim'],
                                                                                                  agent_params['n_layers'],
                                                                                                  agent_params['size'],
                                                                                                  agent_params['discrete'],
                                                                                                  agent_params['learning_rate'],
                                                                                                  env,
                                                                                                  agent_params,
                                                                                                  self.exploration_critic)

        self.actor = ActorPolicy(agent_params['ac_dim'],
                                 agent_params['ob_dim'],
                                 agent_params['n_layers'],
                                 agent_params['size'],
                                 agent_params['discrete'],
                                 agent_params['learning_rate'],
                                 env,
                                 agent_params,
                                 self.critic)

        lander = agent_params['env_name'].startswith('LunarLander')

        self.replay_buffer = MemoryOptimizedReplayBuffer(
            agent_params['replay_buffer_size'], agent_params['frame_history_len'], lander=lander)
        self.t = 0
        self.num_param_updates = 0

    def add_to_replay_buffer(self, paths):
        pass

    def apply_noise(self, input, noise_ratio):
        b = self.env.observation_space.high
        a = self.env.observation_space.low
        if np.inf in b:
            b[:] = 1
        # n = np.random.normal(np.zeros(input.shape, dtype=np.float32), scale=np.sqrt(np.square(input)) * noise_ratio)
        n = np.random.normal(np.zeros(input.shape, dtype=np.float32), scale=b * noise_ratio)
        n = n.astype('float32')
        return input + n

    def step_env(self):
        """
            Step the env and store the transition
            At the end of this block of code, the simulator should have been
            advanced one step, and the replay buffer should contain one more transition.
            Note that self.last_obs must always point to the new latest observation.
        """

        self.replay_buffer_idx = self.replay_buffer.store_frame(self.last_obs, self.agent_params['ac_dim'])
        eps = self.exploration.value(self.t)

        perform_random_action = self.t < self.learning_starts
        explore = self.t < self.num_exploration_steps
        obv = self.last_obs
        if perform_random_action:
            if self.discrete:
                action = int(np.random.rand() * self.num_actions)
            else:
                b = self.env.action_space.high
                a = self.env.action_space.low
                action = np.random.rand(self.num_actions) * (b - a) + a
        else:
            if explore:
                action = self.expl_actor.get_action(ptu.from_numpy(obv)).reshape((self.actor.ac_dim) if not self.discrete else -1)
                if self.discrete:
                    action = action.item()
            else:
                action = self.actor.get_action(ptu.from_numpy(obv)).reshape((self.actor.ac_dim) if not self.discrete else -1)
                if self.discrete:
                    action = action.item()

        self.last_obs, reward, done, info = self.env.step(action)

        self.last_obs = self.apply_noise(self.last_obs, self.noise_ratio)

        self.replay_buffer.store_effect(self.replay_buffer_idx, action, reward, done)

        if done:
            self.last_obs = self.apply_noise(self.env.reset(), self.noise_ratio)
        return obv, action, reward, self.last_obs, done

    def sample(self, batch_size):
        if self.replay_buffer.can_sample(self.batch_size):
            return self.replay_buffer.sample(batch_size)
        else:
            return [], [], [], [], []

    def train(self, batch_a, batch_b):
        log = {}
        if (self.t > self.learning_starts
                and self.t % self.learning_freq == 0
                and self.replay_buffer.can_sample(self.batch_size)
        ):

            log_c = self.critic.update(batch_a, batch_b, self.actor)



            if self.num_param_updates % self.target_update_freq == 0:
                self.critic.update_target_network()

            ob_no_a, ac_na_a, re_a, next_ob_no_a, terminal_n_a = self.unwrap_path(batch_a)


            ob_no_b, ac_na_b, re_b, next_ob_no_b, terminal_n_b = self.unwrap_path(batch_b)


            ## EXPLORATION
            explore = self.t < self.num_exploration_steps
            if explore:
                explore_weight = 1 #self.explore_weight_schedule.value(self.t - self.learning_starts)
                exploit_weight = 1 - explore_weight
                expl_bonus = self.normalize(ptu.to_numpy(self.exploration_model(next_ob_no_a).detach().cpu()))
                mixed_reward = explore_weight * ptu.from_numpy(expl_bonus) + exploit_weight * re_a
                log_e1 = self.exploration_critic.update((ob_no_a, ac_na_a, mixed_reward, next_ob_no_a, terminal_n_a),batch_b,self.expl_actor)
                expl_bonus = self.normalize(ptu.to_numpy(self.exploration_model(next_ob_no_b).detach().cpu()))
                mixed_reward = explore_weight * ptu.from_numpy(expl_bonus) + exploit_weight * re_b
                log_e2 = self.exploration_critic.update((ob_no_b, ac_na_b, mixed_reward, next_ob_no_b, terminal_n_b), batch_a, self.expl_actor)

            log_a = self.actor.update(ob_no_a, ob_no_b, ac_na_b, self.critic)
            self.num_param_updates += 1
            log = {'Critic Loss': log_c['Critic Loss'],
                   'Actor Loss': log_a['Actor Loss']}
            if explore:
                log['Exploration Loss'] = log_e1['Critic Loss']/2 + log_e2['Critic Loss']/2
        return log

    def update_t(self):
        self.t += 1

    def unwrap_path(self, batch):
        ob_no, ac_na, reward_n, next_ob_no, terminal_n = batch
        ob_no = ptu.from_numpy(ob_no)
        ac_na = ptu.from_numpy(ac_na)
        next_ob_no = ptu.from_numpy(next_ob_no)
        reward_n = ptu.from_numpy(reward_n)
        terminal_n = ptu.from_numpy(terminal_n)

        if self.discrete:
            ac_na = ac_na[:, 0].unsqueeze(1)
        return ob_no, ac_na, reward_n, next_ob_no, terminal_n

    def normalize(self, arr):
        mean = np.mean(arr, axis=0)
        eps = 1e-4
        std = np.std(arr, axis=0)
        return (arr - mean) / (std + eps)
